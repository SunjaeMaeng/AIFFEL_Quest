{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc70644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) 데이터 가져오기\n",
    "#sklearn.datasets의 load_diabetes에서 데이터를 가져와주세요.\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes=load_diabetes()\n",
    "\n",
    "#diabetes의 data를 df_X에, target을 df_y에 저장해주세요.\n",
    "df_x = diabetes.data\n",
    "df_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1ffabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#(2) 모델에 입력할 데이터 X 준비하기\n",
    "#df_X에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "import numpy as np\n",
    "\n",
    "print(type(df_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93659c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#(3) 모델에 예측할 데이터 y 준비하기\n",
    "# #df_y에 있는 값들을 numpy array로 변환해서 저장해주세요.\n",
    "print(type(df_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538dc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) train 데이터와 test 데이터로 분리하기\n",
    "# #X와 y 데이터를 각각 train 데이터와 test 데이터로 분리해주세요.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x, df_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2cf55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10)\n",
      "(10,) (1,)\n"
     ]
    }
   ],
   "source": [
    "#(5) 모델 준비하기\n",
    "# #입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.\n",
    "# Feature의 갯수가 10개 (w = 10, b = 1)\n",
    "print(train_x.shape) \n",
    "w = np.random.rand(10)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "print(w.shape, b.shape)\n",
    "\n",
    "#모델 함수를 구현해주세요.\n",
    "def model(x, w, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += x[:, i] * w[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677e4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(6) 손실함수 loss 정의하기\n",
    "#손실함수를 MSE 함수로 정의해주세요.\n",
    "# 두 값의 차이의 제곱의 평균\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "def loss(x, w, b, y):\n",
    "    predictions = model(x, w, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0999ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(7) 기울기를 구하는 gradient 함수 구현하기\n",
    "#기울기를 계산하는 gradient 함수를 구현해주세요.\n",
    "def gradient(x, w, b, y):\n",
    "    N = len(x)\n",
    "    \n",
    "    pred_y = model(x, w, b)\n",
    "    \n",
    "    dw = 1/N * 2 * x.T.dot(pred_y - y) \n",
    "    \n",
    "    db = 2 * (pred_y - y).mean()\n",
    "    \n",
    "    return dw, db, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92477bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(8) 하이퍼 파라미터인 학습률 설정하기\n",
    "#학습률, learning rate 를 설정해주세요\n",
    "#만약 학습이 잘 되지 않는다면 learning rate 값을 한번 여러 가지로 설정하며 실험해 보세요.\n",
    "LEARNING_RATE = 0.05 # 변경하며 실험해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49badbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 8684.8429\n",
      "Iteration 20 : Loss 6159.5864\n",
      "Iteration 30 : Loss 5821.2092\n",
      "Iteration 40 : Loss 5749.2080\n",
      "Iteration 50 : Loss 5710.0991\n",
      "Iteration 60 : Loss 5675.4891\n",
      "Iteration 70 : Loss 5641.9170\n",
      "Iteration 80 : Loss 5608.9534\n",
      "Iteration 90 : Loss 5576.5372\n",
      "Iteration 100 : Loss 5544.6526\n",
      "Iteration 110 : Loss 5513.2892\n",
      "Iteration 120 : Loss 5482.4376\n",
      "Iteration 130 : Loss 5452.0884\n",
      "Iteration 140 : Loss 5422.2328\n",
      "Iteration 150 : Loss 5392.8618\n",
      "Iteration 160 : Loss 5363.9666\n",
      "Iteration 170 : Loss 5335.5388\n",
      "Iteration 180 : Loss 5307.5700\n",
      "Iteration 190 : Loss 5280.0519\n",
      "Iteration 200 : Loss 5252.9764\n",
      "Iteration 210 : Loss 5226.3357\n",
      "Iteration 220 : Loss 5200.1218\n",
      "Iteration 230 : Loss 5174.3273\n",
      "Iteration 240 : Loss 5148.9445\n",
      "Iteration 250 : Loss 5123.9662\n",
      "Iteration 260 : Loss 5099.3850\n",
      "Iteration 270 : Loss 5075.1940\n",
      "Iteration 280 : Loss 5051.3861\n",
      "Iteration 290 : Loss 5027.9545\n",
      "Iteration 300 : Loss 5004.8925\n",
      "Iteration 310 : Loss 4982.1936\n",
      "Iteration 320 : Loss 4959.8512\n",
      "Iteration 330 : Loss 4937.8590\n",
      "Iteration 340 : Loss 4916.2108\n",
      "Iteration 350 : Loss 4894.9005\n",
      "Iteration 360 : Loss 4873.9221\n",
      "Iteration 370 : Loss 4853.2697\n",
      "Iteration 380 : Loss 4832.9375\n",
      "Iteration 390 : Loss 4812.9199\n",
      "Iteration 400 : Loss 4793.2112\n",
      "Iteration 410 : Loss 4773.8061\n",
      "Iteration 420 : Loss 4754.6992\n",
      "Iteration 430 : Loss 4735.8851\n",
      "Iteration 440 : Loss 4717.3588\n",
      "Iteration 450 : Loss 4699.1151\n",
      "Iteration 460 : Loss 4681.1491\n",
      "Iteration 470 : Loss 4663.4559\n",
      "Iteration 480 : Loss 4646.0306\n",
      "Iteration 490 : Loss 4628.8687\n",
      "Iteration 500 : Loss 4611.9654\n",
      "Iteration 510 : Loss 4595.3163\n",
      "Iteration 520 : Loss 4578.9168\n",
      "Iteration 530 : Loss 4562.7626\n",
      "Iteration 540 : Loss 4546.8494\n",
      "Iteration 550 : Loss 4531.1730\n",
      "Iteration 560 : Loss 4515.7293\n",
      "Iteration 570 : Loss 4500.5141\n",
      "Iteration 580 : Loss 4485.5236\n",
      "Iteration 590 : Loss 4470.7538\n",
      "Iteration 600 : Loss 4456.2008\n",
      "Iteration 610 : Loss 4441.8609\n",
      "Iteration 620 : Loss 4427.7304\n",
      "Iteration 630 : Loss 4413.8057\n",
      "Iteration 640 : Loss 4400.0832\n",
      "Iteration 650 : Loss 4386.5593\n",
      "Iteration 660 : Loss 4373.2307\n",
      "Iteration 670 : Loss 4360.0940\n",
      "Iteration 680 : Loss 4347.1458\n",
      "Iteration 690 : Loss 4334.3830\n",
      "Iteration 700 : Loss 4321.8023\n",
      "Iteration 710 : Loss 4309.4005\n",
      "Iteration 720 : Loss 4297.1747\n",
      "Iteration 730 : Loss 4285.1218\n",
      "Iteration 740 : Loss 4273.2388\n",
      "Iteration 750 : Loss 4261.5228\n",
      "Iteration 760 : Loss 4249.9710\n",
      "Iteration 770 : Loss 4238.5805\n",
      "Iteration 780 : Loss 4227.3486\n",
      "Iteration 790 : Loss 4216.2726\n",
      "Iteration 800 : Loss 4205.3499\n",
      "Iteration 810 : Loss 4194.5778\n",
      "Iteration 820 : Loss 4183.9538\n",
      "Iteration 830 : Loss 4173.4753\n",
      "Iteration 840 : Loss 4163.1399\n",
      "Iteration 850 : Loss 4152.9452\n",
      "Iteration 860 : Loss 4142.8888\n",
      "Iteration 870 : Loss 4132.9684\n",
      "Iteration 880 : Loss 4123.1816\n",
      "Iteration 890 : Loss 4113.5262\n",
      "Iteration 900 : Loss 4104.0001\n",
      "Iteration 910 : Loss 4094.6010\n",
      "Iteration 920 : Loss 4085.3268\n",
      "Iteration 930 : Loss 4076.1754\n",
      "Iteration 940 : Loss 4067.1448\n",
      "Iteration 950 : Loss 4058.2329\n",
      "Iteration 960 : Loss 4049.4378\n",
      "Iteration 970 : Loss 4040.7576\n",
      "Iteration 980 : Loss 4032.1902\n",
      "Iteration 990 : Loss 4023.7339\n",
      "Iteration 1000 : Loss 4015.3868\n"
     ]
    }
   ],
   "source": [
    "#(9) 모델 학습하기\n",
    "#정의된 손실함수와 기울기 함수로 모델을 학습해주세요.\n",
    "#loss값이 충분히 떨어질 때까지 학습을 진행해주세요.\n",
    "#입력하는 데이터인 X에 들어가는 특성 컬럼들을 몇 개 빼도 괜찮습니다. 다양한 데이터로 실험해 보세요.\n",
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dw, db, pred_y = gradient(train_x, w, b, train_y)\n",
    "    w -= LEARNING_RATE * dw\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(train_x, w, b, train_y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b7b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 4007.1471\n",
      "Iteration 20 : Loss 3999.0130\n",
      "Iteration 30 : Loss 3990.9827\n",
      "Iteration 40 : Loss 3983.0547\n",
      "Iteration 50 : Loss 3975.2271\n",
      "Iteration 60 : Loss 3967.4984\n",
      "Iteration 70 : Loss 3959.8669\n",
      "Iteration 80 : Loss 3952.3310\n",
      "Iteration 90 : Loss 3944.8891\n",
      "Iteration 100 : Loss 3937.5399\n",
      "Iteration 110 : Loss 3930.2816\n",
      "Iteration 120 : Loss 3923.1128\n",
      "Iteration 130 : Loss 3916.0322\n",
      "Iteration 140 : Loss 3909.0382\n",
      "Iteration 150 : Loss 3902.1295\n",
      "Iteration 160 : Loss 3895.3047\n",
      "Iteration 170 : Loss 3888.5624\n",
      "Iteration 180 : Loss 3881.9013\n",
      "Iteration 190 : Loss 3875.3201\n",
      "Iteration 200 : Loss 3868.8176\n",
      "Iteration 210 : Loss 3862.3924\n",
      "Iteration 220 : Loss 3856.0433\n",
      "Iteration 230 : Loss 3849.7692\n",
      "Iteration 240 : Loss 3843.5687\n",
      "Iteration 250 : Loss 3837.4409\n",
      "Iteration 260 : Loss 3831.3844\n",
      "Iteration 270 : Loss 3825.3983\n",
      "Iteration 280 : Loss 3819.4813\n",
      "Iteration 290 : Loss 3813.6324\n",
      "Iteration 300 : Loss 3807.8505\n",
      "Iteration 310 : Loss 3802.1345\n",
      "Iteration 320 : Loss 3796.4835\n",
      "Iteration 330 : Loss 3790.8965\n",
      "Iteration 340 : Loss 3785.3723\n",
      "Iteration 350 : Loss 3779.9101\n",
      "Iteration 360 : Loss 3774.5088\n",
      "Iteration 370 : Loss 3769.1676\n",
      "Iteration 380 : Loss 3763.8856\n",
      "Iteration 390 : Loss 3758.6617\n",
      "Iteration 400 : Loss 3753.4951\n",
      "Iteration 410 : Loss 3748.3850\n",
      "Iteration 420 : Loss 3743.3304\n",
      "Iteration 430 : Loss 3738.3306\n",
      "Iteration 440 : Loss 3733.3846\n",
      "Iteration 450 : Loss 3728.4917\n",
      "Iteration 460 : Loss 3723.6511\n",
      "Iteration 470 : Loss 3718.8619\n",
      "Iteration 480 : Loss 3714.1234\n",
      "Iteration 490 : Loss 3709.4349\n",
      "Iteration 500 : Loss 3704.7956\n",
      "Iteration 510 : Loss 3700.2047\n",
      "Iteration 520 : Loss 3695.6615\n",
      "Iteration 530 : Loss 3691.1654\n",
      "Iteration 540 : Loss 3686.7155\n",
      "Iteration 550 : Loss 3682.3113\n",
      "Iteration 560 : Loss 3677.9521\n",
      "Iteration 570 : Loss 3673.6371\n",
      "Iteration 580 : Loss 3669.3658\n",
      "Iteration 590 : Loss 3665.1375\n",
      "Iteration 600 : Loss 3660.9516\n",
      "Iteration 610 : Loss 3656.8074\n",
      "Iteration 620 : Loss 3652.7044\n",
      "Iteration 630 : Loss 3648.6420\n",
      "Iteration 640 : Loss 3644.6195\n",
      "Iteration 650 : Loss 3640.6364\n",
      "Iteration 660 : Loss 3636.6921\n",
      "Iteration 670 : Loss 3632.7861\n",
      "Iteration 680 : Loss 3628.9178\n",
      "Iteration 690 : Loss 3625.0867\n",
      "Iteration 700 : Loss 3621.2923\n",
      "Iteration 710 : Loss 3617.5340\n",
      "Iteration 720 : Loss 3613.8114\n",
      "Iteration 730 : Loss 3610.1238\n",
      "Iteration 740 : Loss 3606.4709\n",
      "Iteration 750 : Loss 3602.8522\n",
      "Iteration 760 : Loss 3599.2671\n",
      "Iteration 770 : Loss 3595.7153\n",
      "Iteration 780 : Loss 3592.1961\n",
      "Iteration 790 : Loss 3588.7093\n",
      "Iteration 800 : Loss 3585.2543\n",
      "Iteration 810 : Loss 3581.8308\n",
      "Iteration 820 : Loss 3578.4382\n",
      "Iteration 830 : Loss 3575.0762\n",
      "Iteration 840 : Loss 3571.7443\n",
      "Iteration 850 : Loss 3568.4421\n",
      "Iteration 860 : Loss 3565.1693\n",
      "Iteration 870 : Loss 3561.9255\n",
      "Iteration 880 : Loss 3558.7101\n",
      "Iteration 890 : Loss 3555.5230\n",
      "Iteration 900 : Loss 3552.3636\n",
      "Iteration 910 : Loss 3549.2317\n",
      "Iteration 920 : Loss 3546.1269\n",
      "Iteration 930 : Loss 3543.0487\n",
      "Iteration 940 : Loss 3539.9969\n",
      "Iteration 950 : Loss 3536.9711\n",
      "Iteration 960 : Loss 3533.9710\n",
      "Iteration 970 : Loss 3530.9962\n",
      "Iteration 980 : Loss 3528.0464\n",
      "Iteration 990 : Loss 3525.1213\n",
      "Iteration 1000 : Loss 3522.2206\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dw, db, pred_y = gradient(train_x, w, b, train_y)\n",
    "    w -= LEARNING_RATE * dw\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(train_x, w, b, train_y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f40b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 3519.3439\n",
      "Iteration 20 : Loss 3516.4910\n",
      "Iteration 30 : Loss 3513.6616\n",
      "Iteration 40 : Loss 3510.8553\n",
      "Iteration 50 : Loss 3508.0718\n",
      "Iteration 60 : Loss 3505.3109\n",
      "Iteration 70 : Loss 3502.5723\n",
      "Iteration 80 : Loss 3499.8557\n",
      "Iteration 90 : Loss 3497.1609\n",
      "Iteration 100 : Loss 3494.4875\n",
      "Iteration 110 : Loss 3491.8354\n",
      "Iteration 120 : Loss 3489.2041\n",
      "Iteration 130 : Loss 3486.5936\n",
      "Iteration 140 : Loss 3484.0035\n",
      "Iteration 150 : Loss 3481.4335\n",
      "Iteration 160 : Loss 3478.8836\n",
      "Iteration 170 : Loss 3476.3533\n",
      "Iteration 180 : Loss 3473.8425\n",
      "Iteration 190 : Loss 3471.3509\n",
      "Iteration 200 : Loss 3468.8783\n",
      "Iteration 210 : Loss 3466.4245\n",
      "Iteration 220 : Loss 3463.9892\n",
      "Iteration 230 : Loss 3461.5723\n",
      "Iteration 240 : Loss 3459.1736\n",
      "Iteration 250 : Loss 3456.7927\n",
      "Iteration 260 : Loss 3454.4295\n",
      "Iteration 270 : Loss 3452.0839\n",
      "Iteration 280 : Loss 3449.7555\n",
      "Iteration 290 : Loss 3447.4442\n",
      "Iteration 300 : Loss 3445.1499\n",
      "Iteration 310 : Loss 3442.8722\n",
      "Iteration 320 : Loss 3440.6111\n",
      "Iteration 330 : Loss 3438.3663\n",
      "Iteration 340 : Loss 3436.1377\n",
      "Iteration 350 : Loss 3433.9250\n",
      "Iteration 360 : Loss 3431.7281\n",
      "Iteration 370 : Loss 3429.5469\n",
      "Iteration 380 : Loss 3427.3811\n",
      "Iteration 390 : Loss 3425.2306\n",
      "Iteration 400 : Loss 3423.0951\n",
      "Iteration 410 : Loss 3420.9747\n",
      "Iteration 420 : Loss 3418.8690\n",
      "Iteration 430 : Loss 3416.7779\n",
      "Iteration 440 : Loss 3414.7013\n",
      "Iteration 450 : Loss 3412.6391\n",
      "Iteration 460 : Loss 3410.5909\n",
      "Iteration 470 : Loss 3408.5568\n",
      "Iteration 480 : Loss 3406.5366\n",
      "Iteration 490 : Loss 3404.5300\n",
      "Iteration 500 : Loss 3402.5371\n",
      "Iteration 510 : Loss 3400.5575\n",
      "Iteration 520 : Loss 3398.5913\n",
      "Iteration 530 : Loss 3396.6382\n",
      "Iteration 540 : Loss 3394.6981\n",
      "Iteration 550 : Loss 3392.7710\n",
      "Iteration 560 : Loss 3390.8565\n",
      "Iteration 570 : Loss 3388.9547\n",
      "Iteration 580 : Loss 3387.0654\n",
      "Iteration 590 : Loss 3385.1885\n",
      "Iteration 600 : Loss 3383.3238\n",
      "Iteration 610 : Loss 3381.4712\n",
      "Iteration 620 : Loss 3379.6306\n",
      "Iteration 630 : Loss 3377.8020\n",
      "Iteration 640 : Loss 3375.9850\n",
      "Iteration 650 : Loss 3374.1798\n",
      "Iteration 660 : Loss 3372.3861\n",
      "Iteration 670 : Loss 3370.6038\n",
      "Iteration 680 : Loss 3368.8328\n",
      "Iteration 690 : Loss 3367.0731\n",
      "Iteration 700 : Loss 3365.3244\n",
      "Iteration 710 : Loss 3363.5868\n",
      "Iteration 720 : Loss 3361.8600\n",
      "Iteration 730 : Loss 3360.1440\n",
      "Iteration 740 : Loss 3358.4387\n",
      "Iteration 750 : Loss 3356.7440\n",
      "Iteration 760 : Loss 3355.0598\n",
      "Iteration 770 : Loss 3353.3860\n",
      "Iteration 780 : Loss 3351.7225\n",
      "Iteration 790 : Loss 3350.0691\n",
      "Iteration 800 : Loss 3348.4259\n",
      "Iteration 810 : Loss 3346.7927\n",
      "Iteration 820 : Loss 3345.1694\n",
      "Iteration 830 : Loss 3343.5560\n",
      "Iteration 840 : Loss 3341.9523\n",
      "Iteration 850 : Loss 3340.3583\n",
      "Iteration 860 : Loss 3338.7738\n",
      "Iteration 870 : Loss 3337.1989\n",
      "Iteration 880 : Loss 3335.6333\n",
      "Iteration 890 : Loss 3334.0771\n",
      "Iteration 900 : Loss 3332.5301\n",
      "Iteration 910 : Loss 3330.9922\n",
      "Iteration 920 : Loss 3329.4635\n",
      "Iteration 930 : Loss 3327.9438\n",
      "Iteration 940 : Loss 3326.4329\n",
      "Iteration 950 : Loss 3324.9310\n",
      "Iteration 960 : Loss 3323.4378\n",
      "Iteration 970 : Loss 3321.9533\n",
      "Iteration 980 : Loss 3320.4774\n",
      "Iteration 990 : Loss 3319.0101\n",
      "Iteration 1000 : Loss 3317.5513\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dw, db, pred_y = gradient(train_x, w, b, train_y)\n",
    "    w -= LEARNING_RATE * dw\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(train_x, w, b, train_y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00e9db9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 3316.1008\n",
      "Iteration 20 : Loss 3314.6588\n",
      "Iteration 30 : Loss 3313.2250\n",
      "Iteration 40 : Loss 3311.7994\n",
      "Iteration 50 : Loss 3310.3819\n",
      "Iteration 60 : Loss 3308.9725\n",
      "Iteration 70 : Loss 3307.5712\n",
      "Iteration 80 : Loss 3306.1777\n",
      "Iteration 90 : Loss 3304.7922\n",
      "Iteration 100 : Loss 3303.4145\n",
      "Iteration 110 : Loss 3302.0445\n",
      "Iteration 120 : Loss 3300.6822\n",
      "Iteration 130 : Loss 3299.3275\n",
      "Iteration 140 : Loss 3297.9804\n",
      "Iteration 150 : Loss 3296.6408\n",
      "Iteration 160 : Loss 3295.3087\n",
      "Iteration 170 : Loss 3293.9840\n",
      "Iteration 180 : Loss 3292.6666\n",
      "Iteration 190 : Loss 3291.3565\n",
      "Iteration 200 : Loss 3290.0536\n",
      "Iteration 210 : Loss 3288.7579\n",
      "Iteration 220 : Loss 3287.4693\n",
      "Iteration 230 : Loss 3286.1877\n",
      "Iteration 240 : Loss 3284.9132\n",
      "Iteration 250 : Loss 3283.6456\n",
      "Iteration 260 : Loss 3282.3850\n",
      "Iteration 270 : Loss 3281.1312\n",
      "Iteration 280 : Loss 3279.8841\n",
      "Iteration 290 : Loss 3278.6439\n",
      "Iteration 300 : Loss 3277.4103\n",
      "Iteration 310 : Loss 3276.1834\n",
      "Iteration 320 : Loss 3274.9631\n",
      "Iteration 330 : Loss 3273.7493\n",
      "Iteration 340 : Loss 3272.5421\n",
      "Iteration 350 : Loss 3271.3413\n",
      "Iteration 360 : Loss 3270.1469\n",
      "Iteration 370 : Loss 3268.9589\n",
      "Iteration 380 : Loss 3267.7772\n",
      "Iteration 390 : Loss 3266.6018\n",
      "Iteration 400 : Loss 3265.4327\n",
      "Iteration 410 : Loss 3264.2697\n",
      "Iteration 420 : Loss 3263.1129\n",
      "Iteration 430 : Loss 3261.9621\n",
      "Iteration 440 : Loss 3260.8175\n",
      "Iteration 450 : Loss 3259.6788\n",
      "Iteration 460 : Loss 3258.5461\n",
      "Iteration 470 : Loss 3257.4194\n",
      "Iteration 480 : Loss 3256.2985\n",
      "Iteration 490 : Loss 3255.1836\n",
      "Iteration 500 : Loss 3254.0744\n",
      "Iteration 510 : Loss 3252.9710\n",
      "Iteration 520 : Loss 3251.8733\n",
      "Iteration 530 : Loss 3250.7813\n",
      "Iteration 540 : Loss 3249.6950\n",
      "Iteration 550 : Loss 3248.6143\n",
      "Iteration 560 : Loss 3247.5392\n",
      "Iteration 570 : Loss 3246.4696\n",
      "Iteration 580 : Loss 3245.4056\n",
      "Iteration 590 : Loss 3244.3470\n",
      "Iteration 600 : Loss 3243.2938\n",
      "Iteration 610 : Loss 3242.2461\n",
      "Iteration 620 : Loss 3241.2037\n",
      "Iteration 630 : Loss 3240.1666\n",
      "Iteration 640 : Loss 3239.1349\n",
      "Iteration 650 : Loss 3238.1083\n",
      "Iteration 660 : Loss 3237.0871\n",
      "Iteration 670 : Loss 3236.0710\n",
      "Iteration 680 : Loss 3235.0601\n",
      "Iteration 690 : Loss 3234.0543\n",
      "Iteration 700 : Loss 3233.0536\n",
      "Iteration 710 : Loss 3232.0579\n",
      "Iteration 720 : Loss 3231.0673\n",
      "Iteration 730 : Loss 3230.0817\n",
      "Iteration 740 : Loss 3229.1010\n",
      "Iteration 750 : Loss 3228.1253\n",
      "Iteration 760 : Loss 3227.1545\n",
      "Iteration 770 : Loss 3226.1885\n",
      "Iteration 780 : Loss 3225.2274\n",
      "Iteration 790 : Loss 3224.2711\n",
      "Iteration 800 : Loss 3223.3196\n",
      "Iteration 810 : Loss 3222.3729\n",
      "Iteration 820 : Loss 3221.4309\n",
      "Iteration 830 : Loss 3220.4935\n",
      "Iteration 840 : Loss 3219.5608\n",
      "Iteration 850 : Loss 3218.6328\n",
      "Iteration 860 : Loss 3217.7094\n",
      "Iteration 870 : Loss 3216.7906\n",
      "Iteration 880 : Loss 3215.8763\n",
      "Iteration 890 : Loss 3214.9665\n",
      "Iteration 900 : Loss 3214.0612\n",
      "Iteration 910 : Loss 3213.1604\n",
      "Iteration 920 : Loss 3212.2641\n",
      "Iteration 930 : Loss 3211.3722\n",
      "Iteration 940 : Loss 3210.4846\n",
      "Iteration 950 : Loss 3209.6015\n",
      "Iteration 960 : Loss 3208.7226\n",
      "Iteration 970 : Loss 3207.8481\n",
      "Iteration 980 : Loss 3206.9779\n",
      "Iteration 990 : Loss 3206.1119\n",
      "Iteration 1000 : Loss 3205.2502\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    dw, db, pred_y = gradient(train_x, w, b, train_y)\n",
    "    w -= LEARNING_RATE * dw\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(train_x, w, b, train_y)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "303e2dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2722.3786740547243"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(10) test 데이터에 대한 성능 확인하기\n",
    "#test 데이터에 대한 성능을 확인해주세요.\n",
    "#MSE 손실함수값 3000 이하\n",
    "\n",
    "prediction = model(test_x, w, b)\n",
    "mse = loss(test_x, w, b, test_y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e56599ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#(11) 정답 데이터와 예측한 데이터 시각화하기\n",
    "# #x축에는 X 데이터의 첫 번째 컬럼을, y축에는 정답인 target 데이터를 넣어서 모델이 예측한 데이터를 시각화해 주세요.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(test_x[:, 0], test_y)\n",
    "plt.scatter(test_x[:, 0], prediction)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
